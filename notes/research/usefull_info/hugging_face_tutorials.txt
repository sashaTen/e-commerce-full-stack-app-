..1..pipeline can  inference  and   task   of   any  model  on  hub. 
..2..you  have to  prepare   in the   format that   dataset looks   like 
..3..encoder   models   are  good   for    understanding the   txt 
..4..decoders  are good generators 
..5..seq2seq   are  good   for  transforming input  into   output  format
..6..attention   scores relationships   between  the  words   and   compares   
and   lets   know the   machine  what  to    focus   on   
..7..ERT is an architecture while bert-base-cased, 
a set of weights trained by the Google team for the first release of BERT, is a checkpoint.
However, one can say ‚Äúthe BERT model‚Äù and ‚Äúthe bert-base-cased model.‚Äù
..8..huggingface  is    for   training   /loading/ saving  models 


Behind the pipeline  : 
..9..simple   pipeline  has   3   steps :
tokenizer   for   convertsion 
model   with    input  id's  and   logits 
post  preproccess  which is    infrerence  in   natural  text  

..10..preproccess   by tokenizer   must to  be  done the  way  the   data  for   pre-training  was
preproccessed . AutoTokenizer class and its from_pretrained()  is  for that 
.11..checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
..12..below  is the   pytorch    sensors   pre -trained: 
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}


..13..
*Model (retrieve the hidden states)
*ForCausalLM
*ForMaskedLM
*ForMultipleChoice
*ForQuestionAnswering
*ForSequenceClassification
*ForTokenClassification
and others ü§ó

..14..learn  from    docs:
https://huggingface.co/learn/nlp-course/chapter1/1   

..15..repeat   and   keep  learning 
..16..finish Behind the pipeline
when    you learn   something    try to rebuild    every   learning  example   
and  also  you  need to   have   hypotheses  for  each   example and  concept 
and   always   keep   experementing   .  from   small  examples   to  more   complicated  ones.    
..17..on the   hub  description  of the  model   you  can   look  at  dataset   used  for 
pre-training   and then  adapt  your   data   the   format  of  their   dataset   
and then  apply   " AutoTokenizer".
..18..model   section  is  the   last  one   currently 
..19..so   maybe    difining the   model   type    such   as   
modelforclassification  or    automodelforQA is   important  because 
it  adapts  how   tokenizers  should  work   ,  how   data   should   
pre-trained  the   form   of   input  and  output,  it  leads  the  fine-tuning 
by   specifying  types   of  layers   required  like   classifiction   leyers . 
generally  it  adapts   the    work  for   your  case.

..20..https://huggingface.co/docs/transformers/quicktour    this is   for  further   investigation ..
..21..after  getting   the   output   you  have to  preproccess  it   too  so   now    humans   can  understand  it .
..22..automodel.from_pretrained:
it  donwloads the  model's weights 
and config file. 
it looks  into   config  in  order   to  see  the  class  
where  it  find   info   about  how to tune the model
..23..autoconfig   class   is  similar  to    automodel  class
autoconfig.from_pretrained  -    it is   for loading  the config  file 
..24..you  can  train   your   model  and then    save   it   with  
my_model.save_pretrained('model's  name')
and  then   reload   it  and  use    with   : 
from_pretrained('your   model')
..25..  you  can   tokenize   txt  in  many   ways  
..26..word   tokenizer  : 
This works well for common languages like English
We might need to handle contractions ("don't") or abbreviations ("e.g.") differently.
..27..character  based    way  and    hibrid   way.
..28..there  are  more   ways  for  preproccessing   used  by  different  models 
like   bert  or    gpt

..Handling multiple sequences..

fine tuning : Processing the data

..29..i   need to  recall  the   knowledges  about the   backend   so     i   would  not  forget.  
..30..simple   recipe  of  hwo to   train the  model  aka  fine tune :
*load  the checkpoint
*load  the  model 
*load   tokenizers
*preparee  the   dataset  with   tokenizers
*define    loss  and    optimizer   
*evaluate
..31..later   you    will   to  do   pipeline  and fine  tuning   project 
..32..batch  dictionary  is   keys   id's attention masks , labels  and   types   of   id's 
..33..remember  that   you   only   40   dayse    left  and   your goal  is  to   be   junior 
ml  engineer   and    start  experementing   and that's   it   .  
only  then    you    should    thinking  about  bikding   the   fintech   banking 
..34..the   idea   of   success is   to research  and    define  your  smart   goals  for   each  day .
..35..inputs = tokenizer("This is the first sentence.", "This is the second one")=
'input_ids'
'token_type_ids'
'attention_mask'

..36..you  have to   love   challenges  and   problems   
..37..the  more  you  do   today the  less left  for  tommmorow
..38..the   point  is   to  prepare   all  for   "trainer":
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)



..39..you  will  never  knwo  all   so the   key  is to  learn  everyday  
and  solve  problems   everyday and  pusshing    to  limits   
..40..predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)    .  explain  please
..41..I‚Äôm getting the same error in the colab notebook for the NLP Course: Lesson 3 - Fine-tuning model with the Trainer API. In order to get around it you can:

Run pip install accelerate -U in a cell
In the top menu click Runtime ‚Üí Restart Runtime
Do not rerun any cells with !pip install in them
Rerun all the other code cells and you should be good to go!
On a side note, be sure to turn on a GPU for this notebook by clicking Edit ‚Üí Notebook Settings ‚Üí GPU type - from the top menu. This was the next thing that got me : )


..42..how to write   scalable  code ?
think   in   advance   will   this   solution   / proccess  
take  place in  the  future   and then   write   function for  it    .  






