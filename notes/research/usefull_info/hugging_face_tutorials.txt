..1..pipeline can  inference  and   task   of   any  model  on  hub. 
..2..you  have to  prepare   in the   format that   dataset looks   like 
..3..encoder   models   are  good   for    understanding the   txt 
..4..decoders  are good generators 
..5..seq2seq   are  good   for  transforming input  into   output  format
..6..attention   scores relationships   between  the  words   and   compares   
and   lets   know the   machine  what  to    focus   on   
..7..ERT is an architecture while bert-base-cased, 
a set of weights trained by the Google team for the first release of BERT, is a checkpoint.
However, one can say ‚Äúthe BERT model‚Äù and ‚Äúthe bert-base-cased model.‚Äù
..8..huggingface  is    for   training   /loading/ saving  models 


Behind the pipeline  : 
..9..simple   pipeline  has   3   steps :
tokenizer   for   convertsion 
model   with    input  id's  and   logits 
post  preproccess  which is    infrerence  in   natural  text  

..10..preproccess   by tokenizer   must to  be  done the  way  the   data  for   pre-training  was
preproccessed . AutoTokenizer class and its from_pretrained()  is  for that 
.11..checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
..12..below  is the   pytorch    sensors   pre -trained: 
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}


..13..
*Model (retrieve the hidden states)
*ForCausalLM
*ForMaskedLM
*ForMultipleChoice
*ForQuestionAnswering
*ForSequenceClassification
*ForTokenClassification
and others ü§ó

..14..learn  from    docs:
https://huggingface.co/learn/nlp-course/chapter1/1   

..15..repeat   and   keep  learning 
..16..finish Behind the pipeline
when    you learn   something    try to rebuild    every   learning  example   
and  also  you  need to   have   hypotheses  for  each   example and  concept 
and   always   keep   experementing   .  from   small  examples   to  more   complicated  ones.    
..17..on the   hub  description  of the  model   you  can   look  at  dataset   used  for 
pre-training   and then  adapt  your   data   the   format  of  their   dataset   
and then  apply   " AutoTokenizer".
..18..model   section  is  the   last  one   currently 
..19..so   maybe    difining the   model   type    such   as   
modelforclassification  or    automodelforQA is   important  because 
it  adapts  how   tokenizers  should  work   ,  how   data   should   
pre-trained  the   form   of   input  and  output,  it  leads  the  fine-tuning 
by   specifying  types   of  layers   required  like   classifiction   leyers . 
generally  it  adapts   the    work  for   your  case.

..20..https://huggingface.co/docs/transformers/quicktour    this is   for  further   investigation ..
..21..after  getting   the   output   you  have to  preproccess  it   too  so   now    humans   can  understand  it .
..22..automodel.from_pretrained:
it  donwloads the  model's weights 
and config file. 
it looks  into   config  in  order   to  see  the  class  
where  it  find   info   about  how to tune the model
..23..autoconfig   class   is  similar  to    automodel  class
autoconfig.from_pretrained  -    it is   for loading  the config  file 
..24..you  can  train   your   model  and then    save   it   with  
my_model.save_pretrained('model's  name')
and  then   reload   it  and  use    with   : 
from_pretrained('your   model')


