..1..pipeline can  inference  and   task   of   any  model  on  hub. 
..2..you  have to  prepare   in the   format that   dataset looks   like 
..3..encoder   models   are  good   for    understanding the   txt 
..4..decoders  are good generators 
..5..seq2seq   are  good   for  transforming input  into   output  format
..6..attention   scores relationships   between  the  words   and   compares   
and   lets   know the   machine  what  to    focus   on   
..7..ERT is an architecture while bert-base-cased, 
a set of weights trained by the Google team for the first release of BERT, is a checkpoint.
However, one can say ‚Äúthe BERT model‚Äù and ‚Äúthe bert-base-cased model.‚Äù
..8..huggingface  is    for   training   /loading/ saving  models 


Behind the pipeline  : 
..9..simple   pipeline  has   3   steps :
tokenizer   for   convertsion 
model   with    input  id's  and   logits 
post  preproccess  which is    infrerence  in   natural  text  

..10..preproccess   by tokenizer   must to  be  done the  way  the   data  for   pre-training  was
preproccessed . AutoTokenizer class and its from_pretrained()  is  for that 
.11..checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
..12..below  is the   pytorch    sensors   pre -trained: 
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}


..13..
*Model (retrieve the hidden states)
*ForCausalLM
*ForMaskedLM
*ForMultipleChoice
*ForQuestionAnswering
*ForSequenceClassification
*ForTokenClassification
and others ü§ó

..14..learn  from    docs:
https://huggingface.co/learn/nlp-course/chapter1/1   

..15..repeat   and   keep  learning 
..16..finish Behind the pipeline
when    you learn   something    try to rebuild    every   learning  example   
and  also  you  need to   have   hypotheses  for  each   example and  concept 
and   always   keep   experementing   .  from   small  examples   to  more   complicated  ones.    
..17..on the   hub  description  of the  model   you  can   look  at  dataset   used  for 
pre-training   and then  adapt  your   data   the   format  of  their   dataset   
and then  apply   " AutoTokenizer".
..18..model   section  is  the   last  one   currently 
..19..so   maybe    difining the   model   type    such   as   
modelforclassification  or    automodelforQA is   important  because 
it  adapts  how   tokenizers  should  work   ,  how   data   should   
pre-trained  the   form   of   input  and  output,  it  leads  the  fine-tuning 
by   specifying  types   of  layers   required  like   classifiction   leyers . 
generally  it  adapts   the    work  for   your  case.

..20..https://huggingface.co/docs/transformers/quicktour    this is   for  further   investigation ..
..21..after  getting   the   output   you  have to  preproccess  it   too  so   now    humans   can  understand  it .
..22..automodel.from_pretrained:
it  donwloads the  model's weights 
and config file. 
it looks  into   config  in  order   to  see  the  class  
where  it  find   info   about  how to tune the model
..23..autoconfig   class   is  similar  to    automodel  class
autoconfig.from_pretrained  -    it is   for loading  the config  file 
..24..you  can  train   your   model  and then    save   it   with  
my_model.save_pretrained('model's  name')
and  then   reload   it  and  use    with   : 
from_pretrained('your   model')
..25..  you  can   tokenize   txt  in  many   ways  
..26..word   tokenizer  : 
This works well for common languages like English
We might need to handle contractions ("don't") or abbreviations ("e.g.") differently.
..27..character  based    way  and    hibrid   way.
..28..there  are  more   ways  for  preproccessing   used  by  different  models 
like   bert  or    gpt

..Handling multiple sequences..

fine tuning : Processing the data

..29..i   need to  recall  the   knowledges  about the   backend   so     i   would  not  forget.  
..30..simple   recipe  of  hwo to   train the  model  aka  fine tune :
*load  the checkpoint
*load  the  model 
*load   tokenizers
*preparee  the   dataset  with   tokenizers
*define    loss  and    optimizer   
*evaluate
..31..later   you    will   to  do   pipeline  and fine  tuning   project 
..32..batch  dictionary  is   keys   id's attention masks , labels  and   types   of   id's 
..33..remember  that   you   only   40   dayse    left  and   your goal  is  to   be   junior 
ml  engineer   and    start  experementing   and that's   it   .  
only  then    you    should    thinking  about  bikding   the   fintech   banking 
..34..the   idea   of   success is   to research  and    define  your  smart   goals  for   each  day .
..35..inputs = tokenizer("This is the first sentence.", "This is the second one")=
'input_ids'
'token_type_ids'
'attention_mask'

..36..you  have to   love   challenges  and   problems   
..37..the  more  you  do   today the  less left  for  tommmorow
..38..the   point  is   to  prepare   all  for   "trainer":
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)



..39..you  will  never  knwo  all   so the   key  is to  learn  everyday  
and  solve  problems   everyday and  pusshing    to  limits   
..40..predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)    .  explain  please
..41..I‚Äôm getting the same error in the colab notebook for the NLP Course: Lesson 3 - Fine-tuning model with the Trainer API. In order to get around it you can:

Run pip install accelerate -U in a cell
In the top menu click Runtime ‚Üí Restart Runtime
Do not rerun any cells with !pip install in them
Rerun all the other code cells and you should be good to go!
On a side note, be sure to turn on a GPU for this notebook by clicking Edit ‚Üí Notebook Settings ‚Üí GPU type - from the top menu. This was the next thing that got me : )


..42..how to write   scalable  code ?
think   in   advance   will   this   solution   / proccess  
take  place in  the  future   and then   write   function for  it    .  

..43..how to learn   on the  go? :    when    you   encounter   new   concept    in the   example   try  to   really learn  it   
learn  it  slowly   and  try  to   experrmtn    and practice  it 

..44..the   nature   of   programming  is    just    writing   code  for    your   solution  
and try  to   implement   other's   solutions/  code   and   then  workgin   and   tweaking the  code 
untill   it   works   .
..45..today    skip the   full  fine  tuning   and   go for   the  datsets  library  
and then   go againg to   full  fine tuning .
..46..dont  be  scared of   hard work   .  for  example   fintech    company   .  just   dive  in  and   do   .  
ask    -  "how?"  and   not   "can  i  ?". 
..47..study   like  this   :    once   you   understand   the  material    make   for  pratice 
the   task and  do   this   task   ,  like  it s  homework  from   universuity
..48..  you  not   only   to    push  the   model to the hub  but  also design the card  of the model
..49..look   up for  the  data  analysis .txt file  where   you   will  find some   motivation   for the practice and  project.
..50..maybe   you   also   should    be  interessted   in some   field  like the football   analytics 
..51..just    learn    python  a little  again.  
..53..you   will  have   the   chance   to  work   and  learn   python   by doing  simple    mini    data  analysis  projecys 
from  the data_analysis.txt.
or   maybe    hacking  or   crimes   . 
..54..do  not   try  to  finish  .  enjoy the proccess and dont   run away .
how to   preproccess  and  work    with  data  with  Datasets library    :
*the library  is similar  to the   pandas   provides  a  lot  of  features : 
*dataset.shuffle   or   dataset.select    you can   do   choose  the   small  portion    of  dataset 
and  entire   data   for  the  experementing.
*you clean  the  data   
*add new  columns 
*Dataset.map(): process each element in a dataset individually.
*Dataset.unique() function to find the number of unique 
*We can sort this new column with Dataset.sort() 
* add new columns to a dataset is with the Dataset.add_column()
* Dataset.filter() function to remove reviews that contain fewer than 30 words
* %time before the line of code you wish to measure:
*Dataset.map() and batched=True you can change the number of elements in your dataset
*Datasets is designed to be interoperable with libraries such as Pandas, NumPy, PyTorch, TensorFlow, and JAX. 
drug_dataset.set_format("pandas")
drug_dataset.reset_format()
*Arrow	Dataset.save_to_disk()
*load_from_disk("drug-reviews")
....big   data  and  the datasets   
..55..By default, ü§ó Datasets will decompress the files needed to load a dataset.
..56..A simple way to measure memory usage in Python is with the psutil library
..57..import timeit - little speed test
..58.. Datasets provides a streaming feature
 that allows us to download and access elements on the fly, 
 without needing to download the whole dataset. streaming=True

..59..interleave_datasets() function that converts a list of IterableDataset objects into a single IterableDatase
 useful when you‚Äôre trying to combine large datasets

 so  i the  conclusion  when   you   will  work  with huge  dataset  rememeber that   datasets  can  deal 
 with   big  data.

 Creating your own dataset for ex:
 created a dataset of GitHub    issues and comments from the ü§ó Datasets repository.
 so  using   api's    you  also   can   create your   own  dataset .
 ..60..Semantic search with FAISS
....
you  have  not  learned  the  dataset  library   well
today   you will read the tokenizer intro  
and  later    go  back  and  finish    those   chapters 
....
..61..the    key  of research   is   go  deeply  
for  example  there  is   no   point   of  scratching the   surface 
of   everything   and  instead  you do  like  this :
1   research  about  qa   for finance 
2   go  to  for   techniques   how to   improve  them like  
fine  tuning
3   go for  fine tuning  techniques  in  depth  
4   if  you see that  there  are  some  research gaps 
or  issues  with  techniques  try  to  invent   some that 
covers  it .



..62..how to train a new tokenizer on your own corpus of text using the ** Tokenizers** library.
..63..You cannot use your own pretrained tokenizer for a pretrained model
vocabulary for your tokenizer and the vocabulary of pretrained the model  are different.
..64..Tokenizers's purpose: to translate text into data for model.
..65..the  task  is   ti  understand   all   about  qa   code.





